{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand how to get the data and model, we're ready to train the neural network. First, we need to load the data using the technique we've discussed in the first unit. We have incapsulated all data-loading code in `get_data` function, and model construction - into `get_model`, respectively. In order not to clutter this notebook, those functions are defined in separate `kintro.py` file, which we will import here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -Nq https://raw.githubusercontent.com/MicrosoftDocs/tensorflow-learning-path/main/intro-keras/kintro.py\n",
    "from kintro import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand what happens during training, we need to add a little more detail to our neural network visualization:\n",
    "\n",
    "![Basic neural network with details](images/2-basic-nn-with-details.png)\n",
    "\n",
    "Notice that we've added weights $W$ to the connections between layers, and bias $b$ as input to `Dense` layers &mdash; $W$ and $b$ are the neural network's parameters. Our goal when training our network (also known as fitting) is to find the parameters $W$ and $b$ that minimize the differences between the actual and predicted labels for our data. In order to do this, we add a loss function at the end of our network, which compares the actual and predicted values, returning a large number if they're different and a smaller number if they're similar.\n",
    "\n",
    "There are many possible loss functions, but the most frequenly used one for classification is **crossentropy loss**. It compares two probability distributions - the one calculated by our network, and the expected one (which gives probability of 1 to the correct label from the training set). Because out target is a class number, and not a vector, in this sample we'll use the [`SparseCategoricalCrossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy) function, which is provided to us by Keras.\n",
    "\n",
    "> If you use `CategoricalCrossentropy`, label should be encoded using so-called *one-hot encoding*, where class number is represented by a 0/1 vector with 1 at the corresponding position. `SparseCategoricalCrossentropy` allows us to pass class number instead of one-hot-encoded vector, saving on memory and computation time.   \n",
    "\n",
    "When we introduce a `Dense` layer, a linear operation involving the input data and parameters $W$ and $b$ is performed. For example, the top node of the first linear layer performs the following calculation:\n",
    "\n",
    "$$\n",
    "z^1_1 = w^1_{1,1} x_1 + ... + w^1_{1,784} x_{784} + b^1_1\n",
    "$$\n",
    "\n",
    "If we specify a ReLU **activation function**, the output of the linear operation is then passed as input to a ReLU function:\n",
    "\n",
    "$$\n",
    "a^1_1 = ReLU(z^1_1)\n",
    "$$\n",
    "\n",
    "> It is important to have non-linear activation functions in between linear layers, because otherwise a sequence of linear layers would be mathematically equivalent to just one layer. Having non-linearities between layers allows our network to have more expressive power, because it eventually can approximate any non-linear relationship between data.\n",
    "\n",
    "Mathematically speaking, we can now think of our neural network as a function $\\ell$ that takes as input the data $X$, expected labels $y$, and parameters $W$ and $b$, then performs a sequence of operations on that data, and returns a loss. \n",
    "\n",
    "$$\n",
    "\\mathrm{loss} = \\ell(X, y, W, b)\n",
    "$$\n",
    "\n",
    "Our goal is to find the parameters $W$ and $b$ that lead to the lowest possible loss. (We can't change our data $X$ or the corresponding labels $y$ &mdash; they're fixed &mdash; but we can adjust $W$ and $b$.) It turns out that problems of this kind fall in the well-studied mathematical area of optimization. The simplest minimization algorithm is gradient descent, and in this sample we use a variation known as Stochastic Gradient Descent or [`SGD`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD). \n",
    "\n",
    "We're now ready to \"compile\" the model &mdash; this is where we tell it that we want to use the `SGD` optimizer and the `SparseCategoricalCrossentropy` loss function. We also tell the model that we want it to report on the accuracy during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "(train_dataset, test_dataset) = get_data(batch_size)\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "metrics = ['accuracy']\n",
    "model.compile(optimizer, loss_fn, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few details from the code above deserve a quick explanation. \n",
    "\n",
    "Notice that we pass `from_logits=True` to the loss function. This is because the output of our network is not a probability distribution, but just a vector of numbers, where higher number corresponds to higher probability. To get probabilities, we need to normalize output vector, which is typically done using **softmax** function. Specifying `from_logits=True` automatically calculates the softmax during computation of the loss.\n",
    "\n",
    "Notice also that we pass a `learning_rate` to the `SGD` optimizer. The learning rate is a parameter needed in the gradient descent algorithm. We could have left it at the default, which is 0.01, but it's important to know how to specify it because different learning rates can lead to very different prediction accuracies.\n",
    "\n",
    "Finally, notice that we specified a `batch_size`, which we used in the construction of the `Dataset`, as we saw earlier. This is important during training, because it tells the model that we want to train on 64 images at a time. You might be wondering why 64? Why not train on a single image at a time? Or all 60,000 images at once? Doing a complete training step for each individual image would be inefficient because we would have to perform all the calculations 60,000 times in order to account for every input image. If we included all the input images in $X$, we'd need a lot of memory, and we'd spend a lot of time computing each training step. So we settle for a size in between, called the \"mini-batch\" size. \n",
    "\n",
    "Now that we've configured our model with the parameters we need for training, we can call `fit` to train the model. We specify the number of epochs as 2, which means that we want to iterate over the complete set of 60,000 training images twice while training the neural network. We use just two epochs in this sample because we don't want to spend too much time training the network in a demo. In a real project, you will want to increase the number of epochs, which will lead to better accuracy of the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting:\n",
      "Epoch 1/2\n",
      "938/938 [==============================] - 7s 2ms/step - loss: 0.5589 - accuracy: 0.7991\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.3956 - accuracy: 0.8547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff5342b0760>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 2\n",
    "print('\\nFitting:')\n",
    "model.fit(train_dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training has found values for the parameters $W$ and $b$ such that, when we provide an image as input, we'll get a reasonable prediction as output. Our model is now ready to be tested. Remember that when we loaded the data, we obtained two datasets, one with training data and another with test data. It's time to use the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating:\n",
      "157/157 [==============================] - 1s 2ms/step - loss: 0.4100 - accuracy: 0.8482\n",
      "\n",
      "Test accuracy: 84.8%, test loss: 0.409994\n"
     ]
    }
   ],
   "source": [
    "print('\\nEvaluating:')\n",
    "(test_loss, test_accuracy) = model.evaluate(test_dataset)\n",
    "print(f'\\nTest accuracy: {test_accuracy * 100:>0.1f}%, test loss: {test_loss:>8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can also pass test data directly to fit function to calculate test loss and accuracy after each epoch of training, by specifying  `validation_data=test_dataset` parameter.\n",
    "\n",
    "Ideally, the test loss and accuracy will be pretty similar to the training loss and accuracy printed earlier. If not, we might need to adjust our model or data.\n",
    "\n",
    "> Low training accuracy and high test accuracy typically indicate a situation called **overfitting**. It means that our model has learn the specifics of training data well, but it cannot generalize to other data of the same kind. In order to overcome overfitting, you either need to provide more data to the model during training, or make the model smaller (decrease the number of layers of neurons in each layer). \n",
    "\n",
    "Assuming that we got good test accuracy, we're done with training and can now save the parameters $W$ and $b$ that were calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('outputs/weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our neural network has appropriate values for its parameters, we can use it to make a prediction."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7d8d32a02de2fe32a77a4e581138922e011c09664b6c2991156e76c4176efab"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "conda-env-py38_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
