{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Attention mechanisms and transformers\n",
        "\n",
        "One major drawback of recurrent networks is that all the words in a sequence have the same impact on the result. This causes suboptimal performance with standard LSTM encoder-decoder models for sequence-to-sequence tasks, such as named entity recognition and machine translation.\n",
        "\n",
        "For example, machine translation is implemented by two recurrent networks, where one network, the **encoder**, incorporates the input sequence into the hidden state, and another one, the **decoder**, unrolls this hidden state into the translated result. The problem with this approach is that the final state of the network has a hard time remembering the beginning of the sentence, which causes poor quality results on long sentences.\n",
        "\n",
        "**Attention mechanisms** attempt to fix that problem by weighing the contextual impact of each input vector on each output prediction of the RNN. This is implemented by creating weighted connections between intermediate states. As you can see in the image below, when generating an output symbol $y_t$, we take into account input hidden states $h_i$ with different weight coefficients $\\alpha_{t,i}$. \n",
        "\n",
        "![Image showing an encoder/decoder model with an additive attention layer](notebooks/images/encoder-decoder-attention.png)\n",
        "*The encoder-decoder model with additive attention mechanism in [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf), cited from [this blog post](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)*\n",
        "\n",
        "The attention matrix $\\{\\alpha_{i,j}\\}$ contains information about the impact of each input word in the generation of each output word. Below is a representation of an attention matrix:\n",
        "\n",
        "![Image showing a sample alignment found by RNNsearch-50, taken from Bahdanau - arviz.org](notebooks/images/bahdanau-fig-3.png)\n",
        "\n",
        "*Figure taken from [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) (Fig.3)*\n",
        "\n",
        "Attention mechanisms have contributed to much of the current or near current state of the art in NLP. However, adding attention greatly increases the number of model parameters, which leads to issues with scaling RNNs. In addition, because of the recurrent nature of the models, each element of a sequence needs to be processed in sequential order, which means that training cannot be easily parallelized.\n",
        "\n",
        "The popularity of attention mechanisms combined with their main drawback led to the creation of **transformer models**, such as BERT and OpenGPT3.\n",
        "\n",
        "## Transformer models\n",
        "\n",
        "Instead of forwarding the context of each previous prediction into the next evaluation step, **transformer models** use **positional encodings** and **attention** to capture the context of a given input within a provided window of text. The image below shows how positional encodings with attention can capture context within a given window.\n",
        "\n",
        "![Animated GIF showing how the evaluations are performed in transformer models.](notebooks/images/transformer-animated-explanation.gif) \n",
        "\n",
        "Since each input position is mapped independently to each output position, transformers can parallelize better than RNNs, which enables larger and more expressive language models. Each attention head can be used to learn different relationships between words, which improves downstream NLP tasks.\n",
        "\n",
        "## Building a classification model based on transformer block\n",
        "\n",
        "Before understanding transformer language models as a whole, let's start with a **transformer block**.\n",
        "Keras doesn't contain a built-in Transformer layer, but we can build our own. As before, we'll focus \n",
        "on text classification of AG News dataset. However, Transformer models show more impressive results when\n",
        "used to solve more difficult NLP tasks. \n",
        "\n",
        "> For the sandbox environment, we need to run the following cell to make sure the required library is installed, and the data is prefetched. If you're running locally, you can skip the following cell."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install --quiet tensorflow_datasets\n",
        "!cd ~ && wget -q -O - https://mslearntensorflowlp.blob.core.windows.net/data/tfds-ag-news.tgz | tar xz"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "ds_train, ds_test = tfds.load('ag_news_subset').values()\n",
        "\n",
        "def extract_text(x):\n",
        "    return x['title']+' '+x['description']\n",
        "\n",
        "def tupelize(x):\n",
        "    return (extract_text(x),x['label'])"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "New layers in Keras should subclass the `Layer` class, and implement the `call` method. Let's start by implementing a **positional embedding** layer. We'll use [code from the official Keras documentation](https://keras.io/examples/nlp/text_classification_with_transformer/), and we'll assume that we pad all input sequences to length `maxlen`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "        self.maxlen = maxlen\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = self.maxlen\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x+positions"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This layer consists of two `Embedding` layers: one for embedding tokens (created using techniques we've discussed before) and another for token positions. Token positions are created as a sequence of natural numbers from 0 to `maxlen` using `tf.range`, and then passed through the embedding layer. The two resulting embedding vectors are then added, producing a positionally-embedded reporesentation of the input of shape `maxlen`$\\times$`embed_dim`.\n",
        "\n",
        "<img src=\"notebooks/images/pos-embedding.png\" width=\"40%\"/>\n",
        "\n",
        "Now let's implement the transformer block, which takes as input the output of the previously defined \n",
        "embedding layer and produces an attention vector. We'll use a `MultiHeadAttention` layer which is \n",
        "included in the `tensorflow_addons` library:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_addons==0.13.0"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons==0.13.0\n",
            "  Downloading tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679 kB)\n",
            "\u001b[K     |████████████████████████████████| 679 kB 46.3 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting typeguard>=2.7\n",
            "  Downloading typeguard-2.12.1-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.13.0 typeguard-2.12.1\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_addons as tfa\n",
        "\n",
        "class TransformerBlock(keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = tfa.layers.MultiHeadAttention(num_heads=num_heads, head_size=embed_dim, name='attn')\n",
        "        self.ffn = keras.Sequential(\n",
        "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = keras.layers.Dropout(rate)\n",
        "        self.dropout2 = keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att([inputs, inputs])\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transformer layer applies `MultiHeadAttention` to the positionally-encoded input to produce the attention vector of dimension `maxlen`$\\times$`embed_dim`, which is then mixed with input and normalized using a `LayerNormalizaton` layer.\n",
        "\n",
        "> **Note**: `LayerNormalization` is similar to the `BatchNormalization` layer discussed in the *Computer Vision* part of this learning path, but it normalizes outputs of the previous layer for each training sample independently, to ensure they're in the range [-1..1].\n",
        "\n",
        "The output of this layer is then passed through a `Dense` network (in our case, a two-layer perceptron), and the result is added to the final output (which undergoes normalization again).\n",
        "\n",
        "<img src=\"notebooks/images/transformer-layer.png\" width=\"30%\" />\n",
        "\n",
        "We're now ready to define the classification model using the transformer block:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "maxlen = 256\n",
        "vocab_size = 20000\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(1,)),\n",
        "    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),\n",
        "    TransformerBlock(embed_dim, num_heads, ff_dim),\n",
        "    keras.layers.GlobalAveragePooling1D(),\n",
        "    keras.layers.Dropout(0.1),\n",
        "    keras.layers.Dense(20, activation=\"relu\"),\n",
        "    keras.layers.Dropout(0.1),\n",
        "    keras.layers.Dense(4, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "text_vectorization (TextVect (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "token_and_position_embedding (None, 256, 32)           648192    \n",
            "_________________________________________________________________\n",
            "transformer_block (Transform (None, 256, 32)           10464     \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 20)                660       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 4)                 84        \n",
            "=================================================================\n",
            "Total params: 659,400\n",
            "Trainable params: 659,400\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training tokenizer')\n",
        "model.layers[0].adapt(ds_train.map(extract_text))\n",
        "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
        "model.fit(ds_train.map(tupelize).batch(64),validation_data=ds_test.map(tupelize).batch(64))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training tokenizer\n",
            "1875/1875 [==============================] - 45s 24ms/step - loss: 0.4053 - acc: 0.8532 - val_loss: 0.2594 - val_acc: 0.9147\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fa2c9e322d0>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT transformer models\n",
        "\n",
        "What we have seen above is the usage of transformer block for classification. However, the main power\n",
        "of transformer models is their use in **language modelling**. The main idea is to start with a raw\n",
        "unlabeled text, and try to build a model that will predict some masked words in the text. This will allow the model\n",
        "to learn the overall structure of the language on very large datasets.\n",
        "\n",
        "Complete transformer model, in addition to transformer block discussed above, which is called **encoder**, contains\n",
        "**decoder** that is responsible for prediction of masked tokens.\n",
        "\n",
        "**BERT** (Bidirectional Encoder Representations from Transformers) is a very large multi layer transformer network with 12 layers for *BERT-base*, and 24 for *BERT-large*. The model is first pretrained on a large corpus of text data (Wikipedia + books) using unsupervised training (predicting masked words in a sentence). During pretraining the model absorbs a significant level of language understanding which can then be fine tuned using other datasets. This process is called **transfer learning**. \n",
        "\n",
        "![picture from http://jalammar.github.io/illustrated-bert/](notebooks/images/jalammar-bert-language-modeling-masked-lm.png)\n",
        "\n",
        "There are many variations of transformer architectures including BERT, DistilBERT, BigBird, OpenGPT3 and more that can be fine tuned. \n",
        "\n",
        "Let's see how we can use a pretrained BERT model for solving our traditional sequence classification problem. We'll borrow the idea and some code from the [official documentation](https://www.tensorflow.org/text/tutorials/classify_text_with_bert).\n",
        "\n",
        "To load pretrained models, we'll use the **TensorFlow hub**. We need to make sure that all required libraries are installed:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install -q tensorflow_hub\n",
        "!{sys.executable} -m pip install --no-deps -q tensorflow_text==2.3"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's load the BERT-specific vectorizer.\n",
        "\n",
        "> In the sandbox environment, we need to prefetch the weights for the pretrained BERT network and the vectorizer. If you're running the code locally, you can skip the next cell. Also, this cell is likely to cause problems on non-UNIX machines."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /tmp && wget -q -O - https://mslearntensorflowlp.blob.core.windows.net/models/tfhub_modules.tgz | tar xz"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_text \n",
        "import tensorflow_hub as hub\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "vectorizer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how the vectorizer works:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer(['I love transformers'])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "{'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n       dtype=int32)>,\n 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n array([[  101,  1045,  2293, 19081,   102,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0]], dtype=int32)>,\n 'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n       dtype=int32)>}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's important that you use the same vectorizer as the one that the original network was trained on. The BERT vectorizer returns three components:\n",
        "* `input_word_ids`, which is a sequence of token numbers for the input sentence.\n",
        "* `input_mask`, showing which part of the sequence contains actual input, and which one is padding. It's similar to the mask produced by the `Masking` layer.\n",
        "* `input_type_ids` is used for language modeling tasks, and allows to specify two input sentences in one sequence.\n",
        "\n",
        "Then, we can instantiate the BERT feature extractor:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "bert = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1')"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "z = bert(vectorizer(['I love transformers']))\n",
        "for i,x in z.items():\n",
        "    print(f\"{i} -> { len(x) if isinstance(x, list) else x.shape }\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sequence_output -> (1, 128, 128)\n",
            "pooled_output -> (1, 128)\n",
            "encoder_outputs -> 4\n",
            "default -> (1, 128)\n"
          ]
        }
      ],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BERT layer returns a number of useful results:\n",
        "* `pooled_output` is the result of averaging out all tokens in the sequence. You can view it as an intelligent semantic embedding of the whole network. It's equivalent to the output of `GlobalAveragePooling1D` layer in our previous model.\n",
        "* `sequence_output` is the output of the last transformer layer (corresponds to the output of `TransformerBlock` in our model above).\n",
        "* `encoder_outputs` are the outputs of all transformer layers. Since we've loaded a 4-layer BERT model (as you can probably guess from the name, which contains `4_H`), it has 4 tensors. The last one is the same as `sequence_output`.\n",
        "\n",
        "Now we'll define the end-to-end classification model. We'll use the *functional model definition*, where we define the model input, and then provide a series of expressions to calculate its output. We will also make the BERT model weights non-trainable, and train just the final classifier:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "inp = keras.Input(shape=(),dtype=tf.string)\n",
        "x = vectorizer(inp)\n",
        "x = bert(x)\n",
        "x = keras.layers.Dropout(0.1)(x['pooled_output'])\n",
        "out = keras.layers.Dense(4,activation='softmax')(x)\n",
        "model = keras.models.Model(inp,out)\n",
        "bert.trainable = False\n",
        "model.summary()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None,)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        {'input_mask': (None 0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer_1 (KerasLayer)      {'sequence_output':  4782465     keras_layer[0][0]                \n",
            "                                                                 keras_layer[0][1]                \n",
            "                                                                 keras_layer[0][2]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 4)            516         dropout_5[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 4,782,981\n",
            "Trainable params: 516\n",
            "Non-trainable params: 4,782,465\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "execution_count": 14,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
        "model.fit(ds_train.map(tupelize).batch(64),validation_data=ds_test.map(tupelize).batch(64))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 170s 91ms/step - loss: 0.7144 - acc: 0.7347 - val_loss: 0.5450 - val_acc: 0.8046\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fa19d7161d0>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despite the fact that there are few trainable parameters, the process is pretty slow, because the BERT feature extractor is computationally heavy. It looks like we were unable to achieve reasonable accuracy, either due to lack of training, or lack of model parameters.\n",
        "\n",
        "If you want to further experiment with BERT training, you can try to unfreeze some of the BERT weights and train them as well. This requires a very small learning rate, and more care with the training strategy. In this scenario, it's recommended that we use the **AdamW** optimizer. You can also experiment with more advanced optimization strategies with initial **warmup** (the `tf-models-official` package may be helpful). "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "bert.trainable=True\n",
        "model.summary()\n",
        "epochs = 3\n",
        "opt = tfa.optimizers.AdamW(learning_rate=3e-5, weight_decay=1e-5)\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer=opt)\n",
        "model.fit(ds_train.map(tupelize).batch(16),validation_data=ds_test.map(tupelize).batch(16))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None,)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        {'input_mask': (None 0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer_1 (KerasLayer)      {'sequence_output':  4782465     keras_layer[0][0]                \n",
            "                                                                 keras_layer[0][1]                \n",
            "                                                                 keras_layer[0][2]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 128)          0           keras_layer_1[0][5]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 4)            516         dropout_5[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 4,782,981\n",
            "Trainable params: 4,782,980\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n",
            "7500/7500 [==============================] - 377s 50ms/step - loss: 0.3208 - acc: 0.8889 - val_loss: 0.2452 - val_acc: 0.9164\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fa19cc9e4d0>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the training is quite slow. You may want to experiment and train the model for more epochs (5-10) and see if you can get a better result compared to the approaches we've used before.\n",
        "\n",
        "## Huggingface transformers library\n",
        "\n",
        "Another very common (and a bit simpler) way to use Transformer models is the [HuggingFace package](https://github.com/huggingface/), which provides simple building blocks for different NLP tasks. It's available both for TensorFlow and PyTorch, another very popular neural network framework. \n",
        "\n",
        "> **Note**: If you're not interested in seeing how the transformers library works, you may skip to the end of this notebook &mdash; you won't see anything substantially different from what we've done above. We'll be repeating the same steps of training the BERT model using a different library and substantially larger model. Because the process involves some rather long training, you may want to just look through the code.\n",
        "\n",
        "Let's see how our problem can be solved using [Huggingface Transformers](http://huggingface.co)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!{sys.executable} -m pip install -q transformers"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing we need to do is choose the model that we'll be using. In addition to a few built-in models, Huggingface contains an [online model repository](https://huggingface.co/models), where you can find many more pretrained models created by the community. We can load any of these models just by providing a model name, and all required binary files for the model are automatically downloaded.\n",
        "\n",
        "If you want to load your own models, then you can specify the directory that contains all relevant files, including the parameters for the tokenizer, `config.json` file with model parameters, and binary weights.\n",
        "\n",
        "> When running inside the sandbox, we'll need to download the BERT model files manually. If you're running a local copy of the notebook, then you can skip the next cell, and modify the model name accordingly."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - https://mslearntensorflowlp.blob.core.windows.net/models/huggingface-bert.tgz | tar xz"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the model name or model representation on disk, we can instantiate both the model and the tokenizer (those two always go together). Let's start with a tokenizer:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "# To load the model from Internet repository using model name. \n",
        "# Use this if you are running from your own copy of the notebooks\n",
        "bert_model = 'bert-base-uncased' \n",
        "\n",
        "# To load the model from the directory on disk. Use this for Microsoft Learn module.\n",
        "bert_model = './tfbert'\n",
        "\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
        "\n",
        "MAX_SEQ_LEN = 128\n",
        "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `tokenizer` object contains the `encode` function that can be used directly to encode text:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode('TensorFlow is a great framework for NLP')"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": "[101, 23435, 12314, 2003, 1037, 2307, 7705, 2005, 17953, 2361, 102]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 20,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the tokenizer to encode a sequence in a way that's suitable for passing to the model, including the `input_word_ids`, `input_mask` and `input_type_ids` fields. We can also specify that we want TensorFlow tensors by providing the `return_tensors='tf'` argument:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(['Hello, there'],return_tensors='tf')"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": "{'input_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 101, 7592, 1010, 2045,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[1, 1, 1, 1, 1]], dtype=int32)>}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 21,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our case, we'll be using the pretrained BERT model called `bert-base-uncased`. *Uncased* indicates that the model in case-insensitive. \n",
        "\n",
        "When training the model, we need to provide the tokenized sequence as input, so let's include that in the data processing pipeline. Since `tokenizer.encode` is a Python function, we'll call it using `py_function`, as you've seen before:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def process(x):\n",
        "    return tokenizer.encode(x.numpy().decode('utf-8'),return_tensors='tf',padding='max_length',max_length=MAX_SEQ_LEN,truncation=True)[0]\n",
        "\n",
        "def process_fn(x):\n",
        "    s = x['title']+' '+x['description']\n",
        "    e = tf.py_function(process,inp=[s],Tout=(tf.int32))\n",
        "    e.set_shape(MAX_SEQ_LEN)\n",
        "    return e,x['label']"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can load the actual model using the `BertForSequenceClassfication` package. This ensures that our model already has the required architecture for classification, including the final classifier. You'll see a warning message stating that the weights of the final classifier are not initialized, and that the model requires pretraining - that's' perfectly okay, because it's exactly what we are about to do!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model = transformers.TFBertForSequenceClassification.from_pretrained(bert_model,num_labels=4,output_attentions=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at ./tfbert and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "execution_count": 23,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_bert_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bert (TFBertMainLayer)       multiple                  109482240 \n",
            "_________________________________________________________________\n",
            "dropout_43 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           multiple                  3076      \n",
            "=================================================================\n",
            "Total params: 109,485,316\n",
            "Trainable params: 109,485,316\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "execution_count": 24,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see from the `summary()`, the model contains almost 110 million parameters! Presumably, if we want to do a simple classification task on a relatively small dataset, we don't want to train the BERT base layer:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[0].trainable = False\n",
        "model.summary()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_bert_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bert (TFBertMainLayer)       multiple                  109482240 \n",
            "_________________________________________________________________\n",
            "dropout_43 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           multiple                  3076      \n",
            "=================================================================\n",
            "Total params: 109,485,316\n",
            "Trainable params: 3,076\n",
            "Non-trainable params: 109,482,240\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "execution_count": 25,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're ready to begin training!\n",
        "\n",
        "> **Note**: Training a full-scale BERT model can be very time consuming! Thus we will only train it for the first 32 batches. This is just to show how the model training is set up. If you're interested in trying the full-scale training, just remove `steps_per_epoch` and `validation_steps` parameters, and prepare to wait!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile('adam','sparse_categorical_crossentropy',['acc'])\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "model.fit(ds_train.map(process_fn).batch(32),validation_data=ds_test.map(process_fn).batch(32),steps_per_epoch=32,validation_steps=2)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 20s 633ms/step - loss: 1.6303 - acc: 0.2656 - val_loss: 1.3863 - val_acc: 0.2188\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 26,
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fa17c2f6190>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 26,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you increase the number of iterations, train for several epochs, and wait long enough, you can expect that BERT classification gives us the best accuracy! That's because BERT already understands the structure of the language, and we only need to fine-tune the final classifier. However, because BERT is a large model, the whole training process takes a long time, and requires serious computational power! (GPU, and preferably more than one).\n",
        "\n",
        "> **Note:** In our example, we've been using one of the smallest pretrained BERT models. There are larger models that are likely to yield better results."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Takeaway\n",
        "\n",
        "In this unit, we saw very recent model architectures based on **transformers**. We applied them for our text classification task, but similarly, BERT models can be used for entity extraction, question answering, and other NLP tasks.\n",
        "\n",
        "Transformer models represent current state-of-the-art in NLP, and in most cases it should be the first solution you start experimenting with when implementing custom NLP solutions. However, understanding the basic underlying principles of recurrent neural networks discussed in this module is extremely important if you want to build advanced neural models."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "conda-env-py37_tensorflow-py",
      "language": "python",
      "display_name": "py37_tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "conda-env-py37_tensorflow-py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}