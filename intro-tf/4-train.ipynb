{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import gzip\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've learned about tensors, variables, and automatic differentiation, you're ready to learn how to train a neural network using TensorFlow. You've already seen how to train a neural network using Keras in [module 1](../intro-machine-learning-keras) &mdash; in this notebook, we'll re-implement the training loop in TensorFlow. This will help you understand what goes on under the hood a bit better, will give you the opportunity to customize the training loop if you want, and will enable you to debug it.\n",
    "\n",
    "We'll start by including code that gives us the datasets and model that we'll use in the remainder of this notebook. We will use the same FashionMNIST dataset and data loading code as in [module 1](../intro-machine-learning-keras), so feel free to re-visit that module if something is not clear, or have a look [at the source code](https://github.com/MicrosoftDocs/tensorflow-learning-path/blob/main/intro-keras/kintro.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -Nq https://raw.githubusercontent.com/MicrosoftDocs/tensorflow-learning-path/main/intro-keras/kintro.py\n",
    "from kintro import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the following neural network model in our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model() -> tf.keras.Model:\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned in module 1, the goal of training the neural network is to find parameters $W$ and $b$ that minimize the **loss function**, which measures the difference between the actual and predicted labels. We can visualize this in the following diagram:\n",
    "\n",
    "![Basic neural network with details](images/1-basic-nn-with-details.png)\n",
    "\n",
    "We also mentioned that we can think of the neural network as the function $\\ell$ below, and that we use an optimization algorithm to find the parameters $W$ and $b$ that minimize this function.\n",
    "\n",
    "$$\n",
    "\\mathrm{loss} = \\ell(X, y, W, b)\n",
    "$$\n",
    "\n",
    "Let's now dig deeper into what this optimization algorithm might look like. There are many types of optimization algorithms, but in this tutorial we'll cover only the simplest one: the gradient descent algorithm. To implement gradient descent, we iteratively improve our estimates of $W$ and $b$ according to the update formulas below, until the gradients are smaller than a pre-defined threshold $\\epsilon$ (or for a pre-defined number of times):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  W &:= W - \\alpha \\frac{\\partial \\ell}{\\partial W} \\\\\n",
    "  b &:= b - \\alpha \\frac{\\partial \\ell}{\\partial b}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The parameter $\\alpha$ is typically referred to as the \"learning rate,\" and will be defined later in the code. \n",
    "\n",
    "When doing training, we pass a mini-batch of data as input, perform a sequence of calculations to obtain the loss, then propagate back through the network to calculate the derivatives used in the gradient descent formulas above. Once we have the derivatives, we can update the values of the network's parameters $W$ and $b$ according to the formulas. This sequence of steps is the backpropagation algorithm. By performing these calculations several times, our parameters get updated repeatedly, getting more and more accurate each time. \n",
    "\n",
    "In Keras, when we called the function [`fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit), the backpropagation algorithm was executed several times. Here, we'll start by understanding the code that reflects a single pass of the backpropagation algorithm:\n",
    "\n",
    "- a forward pass through the model to compute the predicted value, `y_prime = model(X, training=True)`\n",
    "- a calculation of the loss using a loss function, `loss = loss_fn(y, y_prime)`\n",
    "- a backward pass from the loss function through the model to calculate derivatives, `grads = tape.gradient(loss, model.trainable_variables)`\n",
    "- a gradient descent step to update $W$ and $b$ using the derivatives calculated in the backward pass, `optimizer.apply_gradients(zip(grads, model.trainable_variables))`\n",
    "\n",
    "Here's the complete code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_one_batch(X, y, model, loss_fn, optimizer) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "  with tf.GradientTape() as tape:\n",
    "    y_prime = model(X, training=True)\n",
    "    loss = loss_fn(y, y_prime)\n",
    "\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  return (y_prime, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the code above ensures that the forward calculations are within the `GradientTape`'s scope, just as we saw in the previous notebook. This makes it possible for us to ask the tape for the gradients. \n",
    "\n",
    "The code above works for a single mini-batch, which is typically much smaller than the full set of data (in this sample we use a mini-batch of size 64, out of 60,000 training data items). But we want to execute the backpropagation algorithm for the full set of data. We can do so by iterating through the `Dataset` we created earlier, which, as we saw in module 1, returns a mini-batch per iteration. There are two critical lines in the code below: the `for` loop and the call to the `fit_one_batch` function. The rest of the code just prints the accuracy and loss as the model is being trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(dataset: tf.data.Dataset, model: tf.keras.Model, loss_fn: tf.keras.losses.Loss, \n",
    "optimizer: tf.optimizers.Optimizer) -> None:\n",
    "  batch_count = len(dataset)\n",
    "  loss_sum = 0\n",
    "  correct_item_count = 0\n",
    "  current_item_count = 0\n",
    "  print_every = 100\n",
    "\n",
    "  for batch_index, (X, y) in enumerate(dataset):\n",
    "    (y_prime, loss) = fit_one_batch(X, y, model, loss_fn, optimizer)\n",
    "\n",
    "    y = tf.cast(y, tf.int64)\n",
    "    correct_item_count += (tf.math.argmax(y_prime, axis=1) == y).numpy().sum()\n",
    "\n",
    "    batch_loss = loss.numpy()\n",
    "    loss_sum += batch_loss\n",
    "    current_item_count += len(X)\n",
    "\n",
    "    if ((batch_index + 1) % print_every == 0) or ((batch_index + 1) == batch_count):\n",
    "      batch_accuracy = correct_item_count / current_item_count * 100\n",
    "      print(f'[Batch {batch_index + 1:>3d} - {current_item_count:>5d} items] accuracy: {batch_accuracy:>0.1f}%, loss: {batch_loss:>7f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A complete iteration over all mini-batches in the dataset is called an \"epoch.\" In this sample, we restrict the code to just two epochs for quick execution, but in a real project you would want to set it to a much higher number (to achieve better predictions). The code below also shows the creation of the loss function and optimizer, which we discussed in module 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting:\n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "[Batch 100 -  6400 items] accuracy: 66.0%, loss: 0.685200\n",
      "[Batch 200 - 12800 items] accuracy: 71.5%, loss: 0.637024\n",
      "[Batch 300 - 19200 items] accuracy: 74.3%, loss: 0.662844\n",
      "[Batch 400 - 25600 items] accuracy: 76.0%, loss: 0.484288\n",
      "[Batch 500 - 31968 items] accuracy: 77.2%, loss: 0.515463\n",
      "[Batch 600 - 38368 items] accuracy: 78.2%, loss: 0.674882\n",
      "[Batch 700 - 44768 items] accuracy: 78.9%, loss: 0.415960\n",
      "[Batch 800 - 51168 items] accuracy: 79.5%, loss: 0.366401\n",
      "[Batch 900 - 57568 items] accuracy: 80.1%, loss: 0.397109\n",
      "[Batch 938 - 60000 items] accuracy: 80.3%, loss: 0.500174\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "[Batch 100 -  6400 items] accuracy: 84.9%, loss: 0.549428\n",
      "[Batch 200 - 12800 items] accuracy: 85.1%, loss: 0.281027\n",
      "[Batch 300 - 19200 items] accuracy: 85.3%, loss: 0.535860\n",
      "[Batch 400 - 25600 items] accuracy: 85.2%, loss: 0.491699\n",
      "[Batch 500 - 32000 items] accuracy: 85.3%, loss: 0.400586\n",
      "[Batch 600 - 38400 items] accuracy: 85.4%, loss: 0.299451\n",
      "[Batch 700 - 44768 items] accuracy: 85.4%, loss: 0.386149\n",
      "[Batch 800 - 51168 items] accuracy: 85.5%, loss: 0.434249\n",
      "[Batch 900 - 57568 items] accuracy: 85.6%, loss: 0.281380\n",
      "[Batch 938 - 60000 items] accuracy: 85.6%, loss: 0.258615\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "batch_size = 64\n",
    "epochs = 2\n",
    "\n",
    "(train_dataset, test_dataset) = get_data(batch_size)\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "print('\\nFitting:')\n",
    "for epoch in range(epochs):\n",
    "  print(f'\\nEpoch {epoch + 1}\\n-------------------------------')\n",
    "  fit(train_dataset, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demistifying the Model\n",
    "\n",
    "In this module, we were still using high-level Keras syntax for model definition. However, it is important to understand that the model consists of the same basic concepts that we have already learnt: tensors and variables. For example, a linear `Dense` layer is represented by two variables, `W` and `b`, which contain trainable weights of the layer. The simplest way to define our two-layer dense neural network, equivalent to the one we were using, would be write the following function:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ -6534.038   -2235.3281  -3665.7358   8478.205    4422.8086  -7590.745\n",
      "   -2849.0298  -1029.7246   2818.633    1850.2557]\n",
      " [  6197.5376  -8896.773   -1897.0498  19720.16    -3257.3933 -14803.206\n",
      "    4768.8804  -7887.8174  -1878.3442   7202.9473]], shape=(2, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W1 = tf.Variable(tf.random.normal(shape=(784,512))) # Weight of first layer\n",
    "b1 = tf.Variable(tf.zeros(shape=(512,))) # Bias of first layer\n",
    "W2 = tf.Variable(tf.random.normal(shape=(512,512))) # Weight of the second layer\n",
    "b2 = tf.Variable(tf.zeros(shape=(512))) # Bias of second layer\n",
    "W3 = tf.Variable(tf.random.normal(shape=(512,10))) # Weight of output layer\n",
    "b3 = tf.Variable(tf.zeros(shape=(10,))) # Bias of output layer\n",
    "\n",
    "def mymodel(x):\n",
    "    x = tf.reshape(x,[-1,784]) # flatten\n",
    "    x = tf.matmul(x,W1)+b1\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.matmul(x,W2)+b2\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.matmul(x,W3)+b3\n",
    "    return x\n",
    "\n",
    "random_input = tf.random.normal(shape=(2,28,28))\n",
    "\n",
    "print(mymodel(random_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is designed to operate on minibatches, in the same way as `model` defined above using Keras syntax. The main difference is that Keras framework encapsulates all weights into corresponsing classes, and makes it easy to compose the final model from a sequence of layers. However, behind the scenes the inner machinery of a neural network is pretty simple.\n",
    "\n",
    "We can even try to train our simple model using gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 4909.4169921875\n",
      "Loss = 62.308170318603516\n",
      "Loss = 52.73152160644531\n",
      "Loss = 26.214557647705078\n",
      "Loss = 31.049571990966797\n",
      "Loss = 23.659847259521484\n",
      "Loss = 25.85222053527832\n",
      "Loss = 25.639324188232422\n",
      "Loss = 16.874116897583008\n",
      "Loss = 24.228927612304688\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01 # Learning rate\n",
    "parameters = [W1,b1,W2,b2,W3,b3]\n",
    "\n",
    "for i, (X, y) in enumerate(train_dataset):\n",
    "    # Compute loss and gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_prime = mymodel(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y, y_prime, from_logits=True)\n",
    "        loss = tf.reduce_mean(loss) # average the loss over minibatch\n",
    "        grads = tape.gradient(loss, parameters)\n",
    "    # Now we need to adjust weights. Instead of calling optimizer.apply_gradients,\n",
    "    # We will implement our own gradient descent:\n",
    "    for grad,var in zip(grads,parameters):\n",
    "        var.assign_sub(lr*grad)\n",
    "    # Print progress\n",
    "    if i%100 == 0:\n",
    "        print(f\"Loss = {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we do not compute accuracy in this code, but the fact that loss decreases steadily indicates that the network is training.\n",
    "\n",
    "This example demonstrates that you only need two main things to build a neural network from scratch:\n",
    " * efficient operations on tensors, and \n",
    " * the ability to automatically compute gradients. \n",
    " \n",
    "It is not a problem if you don't immediately understand how this code works - you can always use more high-level concepts like Keras models, existing optimizers, etc. However, it is useful to realize that the main mechanics of a neural network can be programmed in a few lines of low-level code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7d8d32a02de2fe32a77a4e581138922e011c09664b6c2991156e76c4176efab"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "conda-env-py38_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
