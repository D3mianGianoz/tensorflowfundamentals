{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by including the code that gets the data and model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -Nq https://raw.githubusercontent.com/MicrosoftDocs/tensorflow-learning-path/main/intro-keras/kintro.py\n",
    "from kintro import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make a prediction, we need to pass some data to the model, and do a single forward pass through the network to get the prediction. If the code below is unclear to you, make sure you go back to module 1, where we explain it in detail. Remember that, unlike during testing, we don't need to call the loss function because we're no longer interested in evaluating how well the model is doing. Instead, we call `softmax` to convert the values of the output vector into values between 0 and 1, and then get the `argmax` of that vector to get the predicted label index.\n",
    "\n",
    "Similarly to the training and testing sections, once we're done with debugging, we can add a `@tf.function` decorator to get the performance benefits of graph execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def predict(model: tf.keras.Model, X: np.ndarray) -> tf.Tensor:\n",
    "  y_prime = model(X, training=False)\n",
    "  probabilities = tf.nn.softmax(y_prime, axis=1)\n",
    "  predicted_indices = tf.math.argmax(input=probabilities, axis=1)\n",
    "  return predicted_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, during inference, we give the model input data that it hasn't seen before. In this sample, for simplicity, we input the first 3 images of the test dataset instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting:\n",
      "Actual: Bag, Predicted: Bag\n",
      "Actual: Sandal, Predicted: Sandal\n",
      "Actual: Sandal, Predicted: Sandal\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "model = get_model()\n",
    "model.load_weights('outputs/weights')\n",
    "\n",
    "(_, test_dataset) = get_data(batch_size)\n",
    "(X_batch, actual_index_batch) = next(test_dataset.as_numpy_iterator())\n",
    "X = X_batch[0:3, :, :]\n",
    "actual_indices = actual_index_batch[0:3]\n",
    "\n",
    "predicted_indices = predict(model, X)\n",
    "\n",
    "print('\\nPredicting:')\n",
    "for (actual_index, predicted_index) in zip(actual_indices, predicted_indices):\n",
    "  actual_name = labels_map[actual_index]\n",
    "  predicted_name = labels_map[predicted_index.numpy()]\n",
    "  print(f'Actual: {actual_name}, Predicted: {predicted_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7d8d32a02de2fe32a77a4e581138922e011c09664b6c2991156e76c4176efab"
  },
  "kernelspec": {
   "display_name": "py38_tensorflow",
   "language": "python",
   "name": "conda-env-py38_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
